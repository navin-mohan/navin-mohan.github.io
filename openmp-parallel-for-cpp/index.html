<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Playing around with OpenMP and C++ - Parallelizing for-loops | Navin Mohan</title>
<meta name=keywords content>
<meta name=description content="Yesterday, I came across an interesting repository on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.
Well, yes I&rsquo;m aware of the fact that it is possible to perform all three transformations using just STL.
 Map - std::transform Filter - std::copy_if Reduce - std::accumulate  This stackoverflow answer explains them in detail.">
<meta name=author content="Navin Mohan">
<link rel=canonical href=https://navin-mohan.github.io/openmp-parallel-for-cpp/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.5e2b4101351c21e906f398ae96901791830f58d430f96f2659dab7eaef7b3cb7.css integrity="sha256-XitBATUcIekG85iulpAXkYMPWNQw+W8mWdq36u97PLc=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://navin-mohan.github.io/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://navin-mohan.github.io/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=https://navin-mohan.github.io/favicon.ico>
<link rel=apple-touch-icon href=https://navin-mohan.github.io/favicon.ico>
<link rel=mask-icon href=https://navin-mohan.github.io/favicon.ico>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.88.1">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3MBG0NNR14"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-3MBG0NNR14',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Playing around with OpenMP and C++ - Parallelizing for-loops">
<meta property="og:description" content="Yesterday, I came across an interesting repository on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.
Well, yes I&rsquo;m aware of the fact that it is possible to perform all three transformations using just STL.
 Map - std::transform Filter - std::copy_if Reduce - std::accumulate  This stackoverflow answer explains them in detail.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://navin-mohan.github.io/openmp-parallel-for-cpp/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2019-04-19T00:00:00+00:00">
<meta property="article:modified_time" content="2019-04-19T00:00:00+00:00"><meta property="og:site_name" content="Navin Mohan">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Playing around with OpenMP and C++ - Parallelizing for-loops">
<meta name=twitter:description content="Yesterday, I came across an interesting repository on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.
Well, yes I&rsquo;m aware of the fact that it is possible to perform all three transformations using just STL.
 Map - std::transform Filter - std::copy_if Reduce - std::accumulate  This stackoverflow answer explains them in detail.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://navin-mohan.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Playing around with OpenMP and C++ - Parallelizing for-loops","item":"https://navin-mohan.github.io/openmp-parallel-for-cpp/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Playing around with OpenMP and C++ - Parallelizing for-loops","name":"Playing around with OpenMP and C\u002b\u002b - Parallelizing for-loops","description":"Yesterday, I came across an interesting repository on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.\nWell, yes I\u0026rsquo;m aware of the fact that it is possible to perform all three transformations using just STL.\n Map - std::transform Filter - std::copy_if Reduce - std::accumulate  This stackoverflow answer explains them in detail.","keywords":[],"articleBody":"Yesterday, I came across an interesting repository on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.\nWell, yes I’m aware of the fact that it is possible to perform all three transformations using just STL.\n Map - std::transform Filter - std::copy_if Reduce - std::accumulate  This stackoverflow answer explains them in detail.\nBut that is not why I was interested. Before getting in to that let’s take a brief digression.\nThe Beauty of Functions If you’re anything like me and have taken the effort to go through the horrors of functional programming, then you must know these functions(map,reduce,filter) from another context.\n They are called higher order functions, which is just a fancy name for functions that take functions as arguments(or returns one).\nOne thing that is most celebrated in the land of functional programming is , of course, functions and their inherent lack of side effects. The key observation here is that such functions(pure functions) are independent of thier order of execution as they don’t have any side effects. Just keep that in mind as we will be needing it soon.\nThe Problem Coming back to the reason why I got interested in the first place. The library seemed to work fine and it was all generic code but appeared to slow down with large input sizes. The bottleneck was found to be a serial for-loop which is central.\nThis was the point in the commit history where I started.\nBut as we have seen in the previous section, if the functions are pure then we don’t have to apply them in a serial order. Which means we are free to utilize the hardware parallelism while applying those functions.\nSo it got me wondering if there is a better way. Somehow making that for-loop run parallel would solve the issue. Which is exactly what I did.\nThis post is all about the path I took to get a speed up of ~2x on my machine.\nOpenMP and Parallel Programming After some research, it was clear that OpenMP is what I was looking for. It supports C++ through GCC and can be easily enabled by using the pragma omp directives when needed.\nThe best part is that it can parallelize the for-loop with very minimal changes to the original code. We’ll get into the details in a later section.\nBenchmarking code First things first, we don’t want to manually compile everything whenever something changes. That’s why build systems exist so let’s use one.\nI will be using CMake here. If you’re new to CMake I recommend reading this blog post.\nTo perform the actual benchmarking let’s just use the high_resolution_clock from the standard chrono library.\n/* ** test_vec is a large std::vector containing ** random numbers */ // start time auto start_map = std::chrono::high_resolution_clock::now(); // runs the actual transformation transform::mapdouble, decltype(test_vec)(test_vec, test_map_func); // end time auto end_map = std::chrono::high_resolution_clock::now(); // the runtime duration auto duration = end_map - start_map; We’ll be doing this for all three functions and then prints out the results.\nNow that the benchmarking code is out of the way, let’s focus on the CMake file.\nWe need two targets here, a parallel version and a non-parallel version. So that we can see if we’re making any progress.\nLet’s define the non parallel target first.\n# add out target add_executable(benchmark-no-parallel benchmark.cpp)# add the source director as an include directory target_include_directories(benchmark-no-parallel PRIVATE ${CMAKE_SOURCE_DIR})# nothing fancy here target_compile_options(benchmark-no-parallel PRIVATE -Wall)# finally run our target to see the results add_custom_command( TARGET benchmark-no-parallel POST_BUILD COMMAND ./benchmark-no-parallel WORKING_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY} COMMENT \"Running benchmarks on non parallel version...\" )Since we’re using OpenMP all we need to change is the compile options a tad bit. But before that we need to check if we actually have OpenMP available on the system. Luckily, CMake can take care of that in a few lines.\n# check for OpenMP find_package(OpenMP)# if OpenMP is available if(OPENMP_FOUND) add_executable(benchmark-parallel benchmark.cpp) target_include_directories(benchmark-parallel PRIVATE ${CMAKE_SOURCE_DIR})  # include the OpenMP compile flags as well  target_compile_options(benchmark-parallel PRIVATE ${OpenMP_CXX_FLAGS})  # add the OpenMP libraries for linking  target_link_libraries(benchmark-parallel PRIVATE ${OpenMP_CXX_LIBRARIES}) # finally run our target to see the results  add_custom_command( TARGET benchmark-parallel POST_BUILD COMMAND ./benchmark-parallel WORKING_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY} COMMENT \"Running benchmarks on parallel version...\" )# if OpenMP is not available else() message(WARNING \"OpenMP not found. Cannot run benchmarks without it\")endif()And that’s it, we have our build system ready. Now if we run it we’ll be able to see the benchmarks being run for both the targets.\nWe have not yet added OpenMP directives so the results will be more or less the same.\nParallelizing the for-loop As odd as it may seem, this is the easiest part of all. All we need is a little restructuring of the loop construct and the openmp pragma directive.\nStarting with the map function.\n/* ** The Map function */ // non parallel auto new_arr = container(); for(auto it=arr.begin(); it!=arr.end(); it++){ auto new_ele = func(*it); new_arr.insert(new_arr.end(), new_ele); } // parallel auto new_arr = container(arr.size()); size_t len = arr.size(); auto it = arr.begin(); auto new_it = new_arr.begin(); // openmp magic #pragma omp parallel for schedule(static) for(size_t i = 0; i  len; i++){ auto new_ele = func(*(it+i)); *(new_it+i) = new_ele; } You can find more details on the scheduling part here.\nLet’s do filter function.\n/* ** The Filter function */ // non parallel auto new_arr = container(); for(auto it=arr.begin(); it!=arr.end(); it++){ if(func(*it)) new_arr.insert(new_arr.end(), *it); } // parallel auto new_arr = container(); size_t len = arr.size(); auto it = arr.begin(); #pragma omp parallel for schedule(static) for(size_t i = 0; i  len; i++){ if(func(*it)){ /* the following block is a critical section ** we have to tell openmp that no ** two threads can access it at the same time */ #pragma omp critical  {new_arr.insert(new_arr.end(), *(it+i));} } } Lastly, the reduce function. This one’s a bit tricky. As we discussed earlier the function application remains side-effect free can be ran in parallel but the actual reduction part will take at least $$log n$$ serial steps for $$n$$ results. Since we’re assuming that the function application is the most expensive part of the task, the reduction overhead should be outweighed by the overall savings.\nLuckily, OpenMP makes it very easy to use reductions. You can read more about reductions here.\n/* ** The Reduce function */ // non parallel for(auto it=arr.begin(); it!=arr.end(); it++){ initial += func(*it); } // parallel size_t len = arr.size(); auto it = arr.begin(); /* ** Tells openmp to apply a sum reduction on ** the variable initial */ #pragma omp parallel for schedule(static) reduction(+:initial) for(size_t i = 0; i  len; i++){ initial += func(*(it+i)); } And that is it. We have parallelized the for-loops. Let’s run a benchmark test.\nNormal for-loop Results Map : 418ms Reduce: 178ms Filter: 109ms Parallel for-loop Results Map : 216ms Reduce: 85ms Filter: 55ms The tests were performed on my dual core machine with a vector of 10 million double values. Our optimization nearly slashed the runtimes to half the original.\nThe code is available here.\nAlso checkout part 2 of this post here.\n","wordCount":"1186","inLanguage":"en","datePublished":"2019-04-19T00:00:00Z","dateModified":"2019-04-19T00:00:00Z","author":{"@type":"Person","name":"Navin Mohan"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://navin-mohan.github.io/openmp-parallel-for-cpp/"},"publisher":{"@type":"Organization","name":"Navin Mohan","logo":{"@type":"ImageObject","url":"https://navin-mohan.github.io/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://navin-mohan.github.io/ accesskey=h title="Navin Mohan (Alt + H)">Navin Mohan</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://navin-mohan.github.io/about/ title=about>
<span>about</span>
</a>
</li>
<li>
<a href=https://navin-mohan.github.io/contact/ title=contact>
<span>contact</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://navin-mohan.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://navin-mohan.github.io/posts/>Posts</a></div>
<h1 class=post-title>
Playing around with OpenMP and C++ - Parallelizing for-loops
</h1>
<div class=post-meta>April 19, 2019&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Navin Mohan
</div>
</header>
<div class=post-content><p>Yesterday, I came across <a href=https://github.com/georgeshanti/cpp-transformations>an interesting repository</a> on my GitHub feed. It is a simple and functional header-only C++ library that lets you apply transformations like map, reduce and filter to standard C++ containers.</p>
<p>Well, yes I&rsquo;m aware of the fact that it is possible to perform all three transformations using just STL.</p>
<ul>
<li>Map - <a href=http://www.cplusplus.com/reference/algorithm/transform/><code>std::transform</code></a></li>
<li>Filter - <a href=http://www.cplusplus.com/reference/algorithm/copy_if/><code>std::copy_if</code></a></li>
<li>Reduce - <a href=http://www.cplusplus.com/reference/numeric/accumulate/><code>std::accumulate</code></a></li>
</ul>
<p>This <a href=https://stackoverflow.com/questions/40901615/how-to-replicate-map-filter-and-reduce-behaviors-in-c-using-stl>stackoverflow answer</a> explains them in detail.</p>
<p>But that is not why I was interested. Before getting in to that let&rsquo;s take a brief digression.</p>
<h2 id=the-beauty-of-functions>The Beauty of Functions<a hidden class=anchor aria-hidden=true href=#the-beauty-of-functions>#</a></h2>
<p>If you&rsquo;re anything like me and have taken the effort to go through the horrors of functional programming, then you must know these functions(map,reduce,filter) from another context.</p>
<div style=display:flex;justify-content:center>
<img src=/img/2019-04-19-openmp-parallel-for-cpp/monad-meme.jpg alt=Monads decoding=async loading=lazy>
</div>
<p>They are called higher order functions, which is just a fancy name for functions that take functions as arguments(or returns one).</p>
<p>One thing that is most celebrated in the land of functional programming is , of course, functions and their inherent lack of side effects. The key observation here is that such functions(pure functions) are independent of thier order of execution as they don&rsquo;t have any side effects. Just keep that in mind as we will be needing it soon.</p>
<h2 id=the-problem>The Problem<a hidden class=anchor aria-hidden=true href=#the-problem>#</a></h2>
<p>Coming back to the reason why I got interested in the first place. The library seemed to work fine and it was all generic code but appeared to slow down with large input sizes. The bottleneck was found to be a serial for-loop which is central.</p>
<p>This was <a href=https://github.com/navin-mohan/cpp-transformations/tree/53714dd397b62c25b1ac9961562beb2a59154425>the point</a> in the commit history where I started.</p>
<p>But as we have seen in the previous section, if the functions are pure then we don&rsquo;t have to apply them in a serial order. Which means we are free to utilize the hardware parallelism while applying those functions.</p>
<p>So it got me wondering if there is a better way. Somehow making that for-loop run parallel would solve the issue. Which is exactly what I did.</p>
<p>This post is all about the path I took to get a speed up of ~2x on my machine.</p>
<h2 id=openmp-and-parallel-programming>OpenMP and Parallel Programming<a hidden class=anchor aria-hidden=true href=#openmp-and-parallel-programming>#</a></h2>
<p>After some research, it was clear that <a href=https://www.openmp.org>OpenMP</a> is what I was looking for. It supports C++ through GCC and can be easily enabled by using the <code>pragma omp</code> directives when needed.</p>
<p>The best part is that it can parallelize the for-loop with very minimal changes to the original code. We&rsquo;ll get into the details in a later section.</p>
<h2 id=benchmarking-code>Benchmarking code<a hidden class=anchor aria-hidden=true href=#benchmarking-code>#</a></h2>
<p>First things first, we don&rsquo;t want to manually compile everything whenever something changes. That&rsquo;s why build systems exist so let&rsquo;s use one.</p>
<p>I will be using <a href=https://cmake.org/>CMake</a> here. If you&rsquo;re new to CMake I recommend reading <a href=https://pabloariasal.github.io/2018/02/19/its-time-to-do-cmake-right/>this blog post</a>.</p>
<p>To perform the actual benchmarking let&rsquo;s just use the <code>high_resolution_clock</code> from the standard chrono library.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#75715e>/*
</span><span style=color:#75715e>** test_vec is a large std::vector&lt;double&gt; containing 
</span><span style=color:#75715e>** random numbers
</span><span style=color:#75715e>*/</span>

<span style=color:#75715e>// start time 
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> start_map <span style=color:#f92672>=</span> std<span style=color:#f92672>::</span>chrono<span style=color:#f92672>::</span>high_resolution_clock<span style=color:#f92672>::</span>now();

<span style=color:#75715e>// runs the actual transformation
</span><span style=color:#75715e></span>transform<span style=color:#f92672>::</span>map<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>double</span>, <span style=color:#66d9ef>decltype</span>(test_vec)<span style=color:#f92672>&gt;</span>(test_vec, 
test_map_func);

<span style=color:#75715e>// end time
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> end_map <span style=color:#f92672>=</span> std<span style=color:#f92672>::</span>chrono<span style=color:#f92672>::</span>high_resolution_clock<span style=color:#f92672>::</span>now();

<span style=color:#75715e>// the runtime duration
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> duration <span style=color:#f92672>=</span> end_map <span style=color:#f92672>-</span> start_map;
</code></pre></div><p>We&rsquo;ll be doing this for all three functions and then prints out the results.</p>
<p>Now that the benchmarking code is out of the way, let&rsquo;s focus on the CMake file.</p>
<p>We need two targets here, a parallel version and a non-parallel version. So that we can see if we&rsquo;re making any progress.</p>
<p>Let&rsquo;s define the non parallel target first.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmake data-lang=cmake><span style=color:#75715e># add out target
</span><span style=color:#75715e></span>add_executable(<span style=color:#e6db74>benchmark-no-parallel</span> <span style=color:#e6db74>benchmark.cpp</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># add the source director as an include directory
</span><span style=color:#75715e></span>target_include_directories(<span style=color:#e6db74>benchmark-no-parallel</span> <span style=color:#e6db74>PRIVATE</span> <span style=color:#f92672>${</span>CMAKE_SOURCE_DIR<span style=color:#f92672>}</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># nothing fancy here
</span><span style=color:#75715e></span>target_compile_options(<span style=color:#e6db74>benchmark-no-parallel</span> <span style=color:#e6db74>PRIVATE</span> <span style=color:#e6db74>-Wall</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># finally run our target to see the results
</span><span style=color:#75715e></span>add_custom_command(
    <span style=color:#e6db74>TARGET</span> <span style=color:#e6db74>benchmark-no-parallel</span> <span style=color:#e6db74>POST_BUILD</span>
    <span style=color:#e6db74>COMMAND</span> <span style=color:#e6db74>./benchmark-no-parallel</span>
    <span style=color:#e6db74>WORKING_DIRECTORY</span> <span style=color:#f92672>${</span>CMAKE_RUNTIME_OUTPUT_DIRECTORY<span style=color:#f92672>}</span>
    <span style=color:#e6db74>COMMENT</span> <span style=color:#e6db74>&#34;Running benchmarks on non parallel version...&#34;</span>
)<span style=color:#960050;background-color:#1e0010>
</span></code></pre></div><p>Since we&rsquo;re using OpenMP all we need to change is the compile options a tad bit. But before that we need to check if we actually have OpenMP available on the system. Luckily, CMake can take care of that in a few lines.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cmake data-lang=cmake><span style=color:#75715e># check for OpenMP
</span><span style=color:#75715e></span>find_package(<span style=color:#e6db74>OpenMP</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># if OpenMP is available
</span><span style=color:#75715e></span>if(<span style=color:#e6db74>OPENMP_FOUND</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    add_executable(<span style=color:#e6db74>benchmark-parallel</span> <span style=color:#e6db74>benchmark.cpp</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    target_include_directories(<span style=color:#e6db74>benchmark-parallel</span> <span style=color:#e6db74>PRIVATE</span> <span style=color:#f92672>${</span>CMAKE_SOURCE_DIR<span style=color:#f92672>}</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#75715e># include the OpenMP compile flags as well
</span><span style=color:#75715e></span>    target_compile_options(<span style=color:#e6db74>benchmark-parallel</span> <span style=color:#e6db74>PRIVATE</span> <span style=color:#f92672>${</span>OpenMP_CXX_FLAGS<span style=color:#f92672>}</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#75715e># add the OpenMP libraries for linking
</span><span style=color:#75715e></span>    target_link_libraries(<span style=color:#e6db74>benchmark-parallel</span> <span style=color:#e6db74>PRIVATE</span> <span style=color:#f92672>${</span>OpenMP_CXX_LIBRARIES<span style=color:#f92672>}</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    <span style=color:#75715e># finally run our target to see the results
</span><span style=color:#75715e></span>    add_custom_command(
        <span style=color:#e6db74>TARGET</span> <span style=color:#e6db74>benchmark-parallel</span> <span style=color:#e6db74>POST_BUILD</span>
        <span style=color:#e6db74>COMMAND</span> <span style=color:#e6db74>./benchmark-parallel</span>
        <span style=color:#e6db74>WORKING_DIRECTORY</span> <span style=color:#f92672>${</span>CMAKE_RUNTIME_OUTPUT_DIRECTORY<span style=color:#f92672>}</span>
        <span style=color:#e6db74>COMMENT</span> <span style=color:#e6db74>&#34;Running benchmarks on parallel version...&#34;</span>
    )<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span><span style=color:#75715e># if OpenMP is not available
</span><span style=color:#75715e></span>else()<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>    message(<span style=color:#e6db74>WARNING</span> <span style=color:#e6db74>&#34;OpenMP not found. Cannot run benchmarks without it&#34;</span>)<span style=color:#960050;background-color:#1e0010>
</span><span style=color:#960050;background-color:#1e0010></span>endif()<span style=color:#960050;background-color:#1e0010>
</span></code></pre></div><p>And that&rsquo;s it, we have our build system ready. Now if we run it we&rsquo;ll be able to see the benchmarks being run for both the targets.</p>
<p>We have not yet added OpenMP directives so the results will be more or less the same.</p>
<h2 id=parallelizing-the-for-loop>Parallelizing the for-loop<a hidden class=anchor aria-hidden=true href=#parallelizing-the-for-loop>#</a></h2>
<p>As odd as it may seem, this is the easiest part of all. All we need is a little restructuring of the loop construct and the openmp pragma directive.</p>
<p>Starting with the map function.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#75715e>/*
</span><span style=color:#75715e>** The Map function
</span><span style=color:#75715e>*/</span>


<span style=color:#75715e>// non parallel
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> new_arr <span style=color:#f92672>=</span> container();
<span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>auto</span> it<span style=color:#f92672>=</span>arr.begin(); it<span style=color:#f92672>!=</span>arr.end(); it<span style=color:#f92672>++</span>){
    <span style=color:#66d9ef>auto</span> new_ele <span style=color:#f92672>=</span> func(<span style=color:#f92672>*</span>it);
    new_arr.insert(new_arr.end(), new_ele);
}

<span style=color:#75715e>// parallel
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> new_arr <span style=color:#f92672>=</span> container(arr.size());
size_t len <span style=color:#f92672>=</span> arr.size();
<span style=color:#66d9ef>auto</span> it <span style=color:#f92672>=</span> arr.begin();
<span style=color:#66d9ef>auto</span> new_it <span style=color:#f92672>=</span> new_arr.begin();

<span style=color:#75715e>// openmp magic
</span><span style=color:#75715e></span><span style=color:#75715e>#pragma omp parallel for schedule(static)
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span>(size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> len; i<span style=color:#f92672>++</span>){
    <span style=color:#66d9ef>auto</span> new_ele <span style=color:#f92672>=</span> func(<span style=color:#f92672>*</span>(it<span style=color:#f92672>+</span>i));
    <span style=color:#f92672>*</span>(new_it<span style=color:#f92672>+</span>i) <span style=color:#f92672>=</span> new_ele;
}

</code></pre></div><p>You can find more details on the scheduling part <a href=https://stackoverflow.com/questions/10850155/whats-the-difference-between-static-and-dynamic-schedule-in-openmp>here</a>.</p>
<p>Let&rsquo;s do filter function.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#75715e>/*
</span><span style=color:#75715e>** The Filter function
</span><span style=color:#75715e>*/</span>

<span style=color:#75715e>// non parallel
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> new_arr <span style=color:#f92672>=</span> container();
<span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>auto</span> it<span style=color:#f92672>=</span>arr.begin(); it<span style=color:#f92672>!=</span>arr.end(); it<span style=color:#f92672>++</span>){
    <span style=color:#66d9ef>if</span>(func(<span style=color:#f92672>*</span>it))
        new_arr.insert(new_arr.end(), <span style=color:#f92672>*</span>it);
}

<span style=color:#75715e>// parallel
</span><span style=color:#75715e></span><span style=color:#66d9ef>auto</span> new_arr <span style=color:#f92672>=</span> container();
size_t len <span style=color:#f92672>=</span> arr.size();
<span style=color:#66d9ef>auto</span> it <span style=color:#f92672>=</span> arr.begin();
<span style=color:#75715e>#pragma omp parallel for schedule(static)
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span>(size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> len; i<span style=color:#f92672>++</span>){
    <span style=color:#66d9ef>if</span>(func(<span style=color:#f92672>*</span>it)){
        <span style=color:#75715e>/* the following block is a critical section
</span><span style=color:#75715e>        ** we have to tell openmp that no
</span><span style=color:#75715e>        ** two threads can access it at the same time
</span><span style=color:#75715e>        */</span>
        <span style=color:#75715e>#pragma omp critical                
</span><span style=color:#75715e></span>        {new_arr.insert(new_arr.end(), <span style=color:#f92672>*</span>(it<span style=color:#f92672>+</span>i));}
    }
}
</code></pre></div><p>Lastly, the reduce function. This one&rsquo;s a bit tricky. As we discussed earlier the function application remains side-effect free can be ran in parallel but the actual reduction part will take at least $$log n$$ serial steps for $$n$$ results. Since we&rsquo;re assuming that the function application is the most expensive part of the task, the reduction overhead should be outweighed by the overall savings.</p>
<p>Luckily, OpenMP makes it very easy to use reductions. You can read more about reductions <a href=http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-reduction.html>here</a>.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#75715e>/*
</span><span style=color:#75715e>** The Reduce function
</span><span style=color:#75715e>*/</span>

<span style=color:#75715e>// non parallel
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>auto</span> it<span style=color:#f92672>=</span>arr.begin(); it<span style=color:#f92672>!=</span>arr.end(); it<span style=color:#f92672>++</span>){
    initial <span style=color:#f92672>+=</span> func(<span style=color:#f92672>*</span>it);
}

<span style=color:#75715e>// parallel
</span><span style=color:#75715e></span>size_t len <span style=color:#f92672>=</span> arr.size();
<span style=color:#66d9ef>auto</span> it <span style=color:#f92672>=</span> arr.begin();
<span style=color:#75715e>/*
</span><span style=color:#75715e>** Tells openmp to apply a sum reduction on
</span><span style=color:#75715e>** the variable initial
</span><span style=color:#75715e>*/</span>
<span style=color:#75715e>#pragma omp parallel for schedule(static) reduction(+:initial)
</span><span style=color:#75715e></span><span style=color:#66d9ef>for</span>(size_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> len; i<span style=color:#f92672>++</span>){
    initial <span style=color:#f92672>+=</span> func(<span style=color:#f92672>*</span>(it<span style=color:#f92672>+</span>i));
}
</code></pre></div><p>And that is it. We have parallelized the for-loops. Let&rsquo;s run a benchmark test.</p>
<h3 id=normal-for-loop>Normal for-loop<a hidden class=anchor aria-hidden=true href=#normal-for-loop>#</a></h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>Results
Map   : 418ms
Reduce: 178ms
Filter: 109ms
</code></pre></div><h3 id=parallel-for-loop>Parallel for-loop<a hidden class=anchor aria-hidden=true href=#parallel-for-loop>#</a></h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text>Results
Map   : 216ms
Reduce: 85ms
Filter: 55ms
</code></pre></div><p>The tests were performed on my dual core machine with a vector of 10 million double values. Our optimization nearly slashed the runtimes to half the original.</p>
<p>The code is available <a href=https://github.com/navin-mohan/cpp-transformations/tree/f0399d6dff12b107c7b7ddd1464dd862dd616f24>here</a>.</p>
<p>Also checkout part 2 of this post <a href=https://medium.com/@navinmohan/selective-parallelization-using-c-template-metaprogramming-2fd85ca1d717>here</a>.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=next href=https://navin-mohan.github.io/ealing-with-endianness-in-c/>
<span class=title>Next Page »</span>
<br>
<span>Dealing with endianness in C++</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on twitter" href="https://twitter.com/intent/tweet/?text=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops&url=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f&title=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops&summary=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops&source=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f&title=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on whatsapp" href="https://api.whatsapp.com/send?text=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops%20-%20https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Playing around with OpenMP and C++ - Parallelizing for-loops on telegram" href="https://telegram.me/share/url?text=Playing%20around%20with%20OpenMP%20and%20C%2b%2b%20-%20Parallelizing%20for-loops&url=https%3a%2f%2fnavin-mohan.github.io%2fopenmp-parallel-for-cpp%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://navin-mohan.github.io/>Navin Mohan</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>